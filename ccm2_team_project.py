# -*- coding: utf-8 -*-
"""CCM2_Team_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmKn5V2kn7Xm9dxx58G-AYPWg_M2oaTJ

#Team Project - Causal Discovery

##The aim of this project is to use convergence cross mapping to discover feedback causal relationships that popular algorithms like Granger find difficulties to discover. To achieve this, we follow the below steps.



*   Generate synthetic dataset and test and evaluate both baseline algorithms and CCM algorithm
*   Use these tested algorithms on our real world data

We will like to mention that 80% of the content in this project is knowledge gained from class while 20% is taken from external resources.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.

#Restart runtime after successfully running this cell

!pip install dcor
!pip install matplotlib --upgrade
!pip install statsmodels --upgrade
#matplotlib.__version__
!pip install tigramite

!pip install jdc

# Imports
import numpy as np
import matplotlib
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline     
import sklearn
import tigramite
from tigramite import data_processing as pp
from tigramite.toymodels import structural_causal_processes as toys
from tigramite import plotting as tp
from tigramite.pcmci import PCMCI
from tigramite.independence_tests import ParCorr, GPDC, CMIknn, CMIsymb

#############################################################################

#Installing modules for graph generation
!pip install graphviz
!apt install libgraphviz-dev
!pip install pygraphviz




# Install libraries 
!pip install lingam
!pip install igraph
!pip install factor_analyzer
!pip install pygam

# Import libraries
import numpy as np
import pandas as pd
import graphviz
import lingam
from lingam.utils import make_dot, print_causal_directions, print_dagc

############################################################################



#Importing common packages
import os
import math
import glob
import pandas as pd
import numpy as np
import datetime
from scipy import stats
import tensorflow as tf
import seaborn as sns
import random
import matplotlib.pyplot as plt
#Importing packages for graph generation
import networkx as nx
from networkx.drawing.nx_agraph import graphviz_layout, to_agraph
import pygraphviz as pgv
#Importing packages for Granger Causality
from dateutil.parser import parse
from scipy import signal
from scipy.interpolate import interp1d
from scipy import stats
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf, grangercausalitytests
from statsmodels.nonparametric.smoothers_lowess import lowess
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_squared_error

#Importing packages for PCMCI

!pip install causal-learn
from causallearn.search.ConstraintBased.PC import pc
from causallearn.utils.cit import kci, chisq, fisherz
from causallearn.search.ScoreBased.GES import ges
from causallearn.utils.GESUtils import local_score_BIC
from causallearn.utils.GraphUtils import GraphUtils
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import io

from causallearn.search.ConstraintBased.FCI import fci

from causallearn.utils.GraphUtils import GraphUtils

# %matplotlib inline

from random import randrange
from pandas import Series
from matplotlib import pyplot


### Just to remove warnings to prettify the notebook. 
import warnings
warnings.filterwarnings("ignore")

"""### Generating Synthetic Data

```
# This is formatted as code
```

Variable1: S1(t)=0.125√2exp(-S1(t-1)S1(t-1)/2) + 0.3√2exp(-S1(t)S1(t)/2) + 0.2exp(-S5(t-3)S5(t-3)/2) + 0.2exp(-S6(t)S6(t)/2) + ε1

Variable2: S2(t) = 1.2exp(-S1(t-1)S1(t-1)/2) + 0.2exp(-S1(t-2)S1(t-2)/2) + 0.2exp(-S5(t-2)S5(t-2)/2) + 0.2exp(-S3(t-1)S3(t-1)/2) + ε2

Variable3: S3(t)= -1.05exp(-S1(t-1)S1(t-1)/2) + 0.2exp(-S3(t)S3(t)/2) + 0.2exp(-S2(t-2)S2(t-2)/2) + 0.2exp(-S6(t-2)S6(t-2)/2) + ε3

Variable4: S4(t)= -1.15exp(-S1(t-1)S1(t-1)/2) + 0.2 √2 exp(-S4(t-1)S4(t-1)/2) + 1.35 exp(-S3(t-1)*S3(t-1)/2) + ε4

Variable5: S5(t)= -1.15exp(-S1(t-3)S1(t-3)/2) + 0.2 √2 exp(-S2(t-2)S2(t-2)/2) + 1.35 exp(-S3(t-1)*S3(t-1)/2) + ε5

Variable6: S6(t)= -1.05exp(-S1(t-1)S1(t-1)/2) + 0.2exp(-S3(t)S3(t)/2) + 0.2exp(-S2(t-2)S2(t-2)/2) + 0.2exp(-S7(t)S7(t)/2) + ε3

Variable7: S7(t)= -1.05exp(-S4(t-2)S4(t-2)/2) + 0.2exp(-S7(t)S7(t)/2) + 0.2exp(-S5(t-3)S5(t-3)/2) + 0.2exp(-S6(t)S6(t)/2) + ε3

Variable8: S8(t)= -1.05exp(-S7(t-2)S7(t-2)/2) + 0.2exp(-S8(t)S8(t)/2) + 0.2exp(-S6(t-1)S6(t-1)/2) + 0.2exp(-S2(t-3)S2(t-3)/2) + ε3
"""

#Size of time-series: t
t = 100000

#Create noise
import numpy as np
import pandas as pd
np.set_printoptions(suppress=True)
noise = np.random.normal(0,1,t)
print(noise.size)

#Variable 1
source1 = np.zeros((t))
import math
source1[1] = noise[1] + 10
source1[2] = noise[2] + 10
for x in range(3,t):
  source1[x] = 0.125*math.sqrt(2)*math.exp(-source1[x-1]*source1[x-1]/2) + 0.3*math.sqrt(2)*math.exp(-source1[x]*source1[x]/2) + 0.2*math.sqrt(2)*math.exp(-source5[x-3]*source5[x-3]/2) + 0.2*math.sqrt(2)*math.exp(-source6[x]*source6[x]/2) + noise[x]

#Variable 2
source2 = np.zeros((t))
np.set_printoptions(suppress=True)
noise2 = np.random.normal(0,1,t)
noise2
import math
source2[1] = noise2[1]
source2[2] = noise2[2]
for x in range(3,t):
  if source1[x-1] > 0:
     source2[x] = 1.2*math.exp(-source1[x-1]*source1[x-1]/2) + 0.2*math.sqrt(2)*math.exp(-source1[x-2]*source1[x-2]/2) + 0.2*math.sqrt(2)*math.exp(-source5[x-2]*source5[x-2]/2) + 0.2*math.sqrt(2)*math.exp(-source3[x-1]*source3[x-1]/2) + noise2[x]
  else:
     source2[x] = -2*math.exp(-source1[x-1]*source1[x-1]) + noise2[x]

#Variable 3
source3 = np.zeros((t))
np.set_printoptions(suppress=True)
noise3 = np.random.normal(0,1,t)
noise3
import math
source3[1] = noise3[1]
source3[2] = noise3[2]
for x in range(3,t):
  source3[x] = -1.05*math.exp(-source1[x-1]*source1[x-1]/2) + 0.2*math.sqrt(2)*math.exp(-source3[x]*source3[x]/2) + 0.2*math.sqrt(2)*math.exp(-source2[x-2]*source2[x-2]/2) + noise3[x]

#Variable 4
source4 = np.zeros((t))
np.set_printoptions(suppress=True)
noise4 = np.random.normal(0,1,t)
import math
source4[1] = noise4[1] + 10
source4[2] = noise4[2] + 10
for x in range(3,t):
  source4[x] = -1.15*math.exp(-source1[x-1]*source1[x-1]/2) + 0.2*math.sqrt(2)*math.exp(-source4[x-1]*source4[x-1]/2) + 1.35*math.exp(-source3[x-1]*source3[x-1]/2) + noise4[x]


#Variable 5
source5 = np.zeros((t))
np.set_printoptions(suppress=True)
noise5 = np.random.normal(0,1,t)
import math
source5[1] = noise5[1] + 5
source5[2] = noise5[2] + 5
for x in range(3,t):
  source5[x] = -1.15*math.exp(-source1[x-3]*source1[x-3]/2) + 0.2*math.sqrt(2)*math.exp(-source2[x-2]*source2[x-2]/2) + 1.35*math.exp(-source3[x-1]*source3[x-1]/2) + noise5[x]


#Variable 6
source6 = np.zeros((t))
np.set_printoptions(suppress=True)
noise6 = np.random.normal(0,1,t)
noise6
import math
source6[1] = noise6[1]
source6[2] = noise6[2]
for x in range(3,t):
  source6[x] = -1.05*math.exp(-source1[x-1]*source1[x-1]/2) + 0.2*math.sqrt(2)*math.exp(-source3[x]*source3[x]/2) + 0.2*math.sqrt(2)*math.exp(-source2[x-2]*source2[x-2]/2) + 0.2*math.sqrt(2)*math.exp(-source7[x-1]*source7[x-1]/2) + noise6[x]

#Variable 7
source7 = np.zeros((t))
np.set_printoptions(suppress=True)
noise7 = np.random.normal(0,1,t)
noise7
import math
source7[1] = noise7[1]
source7[2] = noise7[2]
for x in range(3,t):
  source7[x] = -1.05*math.exp(-source4[x-2]*source4[x-2]/2) + 0.2*math.sqrt(2)*math.exp(-source7[x]*source7[x]/2) + 0.2*math.sqrt(2)*math.exp(-source5[x-3]*source5[x-3]/2) + 0.2*math.sqrt(2)*math.exp(-source6[x]*source6[x]/2) + noise7[x]


#Variable 8
source8 = np.zeros((t))
np.set_printoptions(suppress=True)
noise8 = np.random.normal(0,1,t)
noise8
import math
source8[1] = noise8[1]
source8[2] = noise8[2]
for x in range(3,t):
  source8[x] = -1.05*math.exp(-source7[x-2]*source7[x-2]/2) + 0.2*math.sqrt(2)*math.exp(-source8[x]*source8[x]/2) + 0.2*math.sqrt(2)*math.exp(-source6[x-1]*source6[x-1]/2) + 0.2*math.sqrt(2)*math.exp(-source2[x-3]*source2[x-3]/2) + noise8[x]


#combining data
dict={'S1':source1,'S2':source2,'S3':source3,'S4':source4,'S5':source5,'S6':source6,'S7':source7,'S8':source8}
data=pd.DataFrame(dict)
data.to_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv',header=True,index=False)
data.to_csv('/content/synthetic_data_nonlinear.csv',header=True,index=False)
from google.colab import files
files.download( "synthetic_data_nonlinear.csv" )

#Loading data and printing out initial records
combined_data = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv')
print(combined_data.info())
print(combined_data.head())

## Build the causal graph and display it
# A directed graph
graph = nx.DiGraph()
# Add edges
graph.add_edge("S1","S1")
graph.add_edge("S5","S1")
graph.add_edge("S6","S1")

graph.add_edge("S1","S2")
graph.add_edge("S5","S2")
graph.add_edge("S3","S2")

graph.add_edge("S1","S3")
graph.add_edge("S3","S3")
graph.add_edge("S2","S3")
graph.add_edge("S6","S3")

graph.add_edge("S1","S4")
graph.add_edge("S4","S4")
graph.add_edge("S3","S4")

graph.add_edge("S1","S5")
graph.add_edge("S2","S5")
graph.add_edge("S3","S5")

graph.add_edge("S1","S6")
graph.add_edge("S2","S6")
graph.add_edge("S3","S6")
graph.add_edge("S7","S6")

graph.add_edge("S4","S7")
graph.add_edge("S7","S7")
graph.add_edge("S5","S7")
graph.add_edge("S6","S7")

graph.add_edge("S7","S8")
graph.add_edge("S8","S8")
graph.add_edge("S6","S8")
graph.add_edge("S2","S8")



# Set layout and draw the causal graph
nx.draw(graph, pos=nx.nx_agraph.graphviz_layout(graph), node_size=1200, font_size=10, node_color='#d4d1c5', with_labels=True)

"""#Constraint - Based Structure Learning

###PC Algorithm###
"""

#df1 = df.drop('date', axis=1)

df1 = combined_data.to_numpy()

# significance level (alpha) at 0.05
# Here I used “fisherz”: Fisher’s Z conditional independence test because there are no missing values in the data, otherwise “mv_fisherz”
cg = pc(df1, 0.05, fisherz, True, 0, -1)

plt.figure(figsize=(12,8))

# visualization using pydot
cols = combined_data.columns
cg.draw_pydot_graph(labels=cols)

"""###Fast Causal Inference - FCI Algorithm###"""

#from causallearn.search.ConstraintBased.FCI import fci

G, edges = fci(df1, fisherz, 0.05, -1, -1, )

# visualization
#from causallearn.utils.GraphUtils import GraphUtils

cols = combined_data.columns
pdy = GraphUtils.to_pydot(G, labels=cols)
#pdy.write_png('simple_test.png')

plt.figure(figsize=(12,8))

# Visualization using pydot
#pyd = GraphUtils.to_pydot(Record['G'], labels=cols)
tmp_png = pdy.create_png(f="png")
fp = io.BytesIO(tmp_png)
img = mpimg.imread(fp, format='png')
plt.axis('off')
plt.imshow(img)
plt.show()

"""#Score-based Causal Discovery
###Greedy Equivalence Search (GES) Algorithm###
"""

Record = ges(df1, score_func = "local_score_BIC", maxP = None, parameters = None)
cols = combined_data.columns

plt.figure(figsize=(12,8))

# Visualization using pydot
pyd = GraphUtils.to_pydot(Record['G'], labels=cols)
tmp_png = pyd.create_png(f="png")
fp = io.BytesIO(tmp_png)
img = mpimg.imread(fp, format='png')
plt.axis('off')
plt.imshow(img)
plt.show()

"""#Granger Causality Test
We will perform a bivariate test on two variables at a time. To identify whether a time-series A granger causes time-series B, we need to set a threshold for p-value, let's say 0.05. If p-value is less than 0.05, we say the null-hypothesis is rejected and A granger causes B. Otherwise, A does not granger cause B.
"""

n_obs=30000
X_train, X_test = combined_data[0:-n_obs], combined_data[-n_obs:]
print(X_train.shape, X_test.shape)

maxlag=12
test = 'ssr-chi2test'
def grangers_causality_matrix(X_train, variables, test = 'ssr_chi2test', verbose=False):
  dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
  for c in dataset.columns:
    for r in dataset.index:
      test_result = grangercausalitytests(X_train[[r,c]], maxlag=maxlag, verbose=False)
      p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
      if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
      
      min_p_value = np.min(p_values)
      dataset.loc[r,c] = min_p_value
  dataset.columns = [var + '_x' for var in variables]
  dataset.index = [var + '_y' for var in variables]
  return dataset
grangers_causality_matrix(combined_data, variables = combined_data.columns)

"""As evident from the results. The p-value of S1_x for S2_y, S3_y, S4_y, S5_y, S6_y, S7_y and S8_y is less than 0.05, which means the Granger test identifies S1 to be the cause of S2, S3 and S4. Similarly, S3 is identified to be the granger cause of S4.

#PCMCI
This methods combines the PC stable causal discovery algorithm with momentary conditional independence MCI to estimate causal networks from large-scale time series datasets in a two-step approach. The first step is to find parents of all individual nodes using the PC stable method. The second step is to test the momentary conditional independence. As a result of the second step, if two nodes are independent, then there exists no cause-effect relationship between them. The outcome of this two-step method is a causal adjacency matrix, the corresponding lag values and strength given by p-values.
Source: https://github.com/jakobrunge/tigramite
"""

df = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv')
data = np.array(df)
var_names=['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8']

dataframe = pp.DataFrame(data, 
                         datatime = {0:np.arange(len(data))}, 
                         var_names=var_names)

#Define the maximum lag and conditional independence test. 
max_lag = 5
parcorr = ParCorr(significance='analytic')
#Create an object of PCMCI class
pcmci = PCMCI(
    dataframe=dataframe, 
    cond_ind_test=parcorr,
    verbosity=1)

pcmci.verbosity = 1
results = pcmci.run_pcmci(tau_max=max_lag, pc_alpha=None, alpha_level=0.01)

"""#### Plotting
The network is defined from links in graph. Nodes denote variables, straight links contemporaneous dependencies and curved arrows lagged dependencies. The node color denotes the maximal absolute auto-dependency and the link color the value at the lag with maximal absolute cross-dependency. The link label lists the lags with significant dependency in order of absolute magnitude.
"""

tp.plot_graph(
    val_matrix=results['val_matrix'],
    graph=results['graph'],
    var_names=var_names,
    link_colorbar_label='cross-MCI',
    node_colorbar_label='auto-MCI',
    ); plt.show()

"""From this graph, we see that PCMCI correctly identifies the time-lag that is '1' and also identifies the causal relations between
- S1 and S2, S3, S4, S5, S6, 
- S3 and S5, S6
- S4 and S7, S5

#VarLiNGAM

VarLiNGAM is a Structural Equation Models (SEM) method of Causal Discovery. It combines the basic LiNGAM model with the classic vector autoregressive models (VAR). It enables analyzing both lagged and contemporaneous (instantaneous) causal relations. Therefore it is allowed to discover linear instantaneous and lagged effects of time-series data.
Please found the documentation of VarLiNGAM: https://lingam.readthedocs.io/en/latest/tutorial/var.html
"""

# Read Data and remove the first line of variable names
X = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv',skiprows = 1, header=None)

# Initialize the model and fit data into the model
# It takes about 4 minitues to run this model with this ysnthetic data
model = lingam.VARLiNGAM(lags=2, criterion=None, prune=True)
model.fit(X)

"""#### Result
VarLiNGAM reports causal relationships in adjacency matrices. The elements in the matrices $B$ are causal effects.
"""

# Print adjacent matrices
print(model.adjacency_matrices_)

"""From the results we can see, VarLiNGAM believes there only causal relationships when time lag is up to 3. It is reasonable, according to our synthetic data.

Let's print a causal plot. The $lower limit = 0.05$ means we only look at causal edges with a causal effect larger than 0.05 and ignore other edges in the graph. 
"""

new_matrices = np.stack((model.adjacency_matrices_[0], model.adjacency_matrices_[1]))
labels = ['S1(t)', 'S2(t)', 'S3(t)', 'S4(t)','S5(t)', 'S6(t)', 'S7(t)', 'S8(t)',
          'S1(t-1)', 'S2(t-1)', 'S3(t-1)', 'S4(t-1)', 'S5(t-1)', 'S6(t-1)', 'S7(t-1)', 'S8(t-1)',
          'S1(t-2)', 'S2(t-2)', 'S3(t-2)', 'S4(t-2)', 'S5(t-2)', 'S6(t-2)', 'S7(t-2)', 'S8(t-2)',
          'S1(t-3)', 'S2(t-3)', 'S3(t-3)', 'S4(t-3)', 'S5(t-3)', 'S6(t-3)', 'S7(t-3)', 'S8(t-3)']
make_dot(np.hstack(new_matrices), ignore_shape=False, lower_limit=0.05, )



#df3 = df1_norm.loc['2017-01-01':'2021-08-31']
model = lingam.VARLiNGAM(lags=5, criterion=None, prune=True)
model.fit(X)

p_values = model.get_error_independence_p_values()
print(p_values)

# model = lingam.VARLiNGAM()
result = model.bootstrap(X, n_sampling=100)

cdc = result.get_causal_direction_counts(n_directions=8, min_causal_effect=0.3, split_by_causal_effect_sign=True)

print_causal_directions(cdc, 100, labels=labels)

"""If we compare with the ground truth.
The ground truth: 


VarLiNGAM discovery:

## CCM Algorithm

### Defining CCM Functions
"""

# Computing "Causality" (Correlation between True and Predictions)
class ccm:
    def __init__(self, X, Y, tau=1, E=2, L=500):
        '''
        X: timeseries for variable X that could cause Y
        Y: timeseries for variable Y that could be caused by X
        tau: time lag
        E: shadow manifold embedding dimension
        L: time period/duration to consider (longer = more data)
        We're checking for X -> Y
        '''
        self.X = X
        self.Y = Y
        self.tau = tau
        self.E = E
        self.L = L        
        self.My = self.shadow_manifold(Y) # shadow manifold for Y (we want to know if info from X is in Y)
        self.t_steps, self.dists = self.get_distances(self.My) # for distances between points in manifold

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# def shadow_manifold(self, X):
#     """
#     Given
#         X: some time series vector
#         tau: lag step
#         E: shadow manifold embedding dimension
#         L: max time step to consider - 1 (starts from 0)
#     Returns
#         {t:[t, t-tau, t-2*tau ... t-(E-1)*tau]} = Shadow attractor manifold, dictionary of vectors
#     """
#     X = X[:L] # make sure we cut at L
#     M = {t:[] for t in range((self.E-1) * self.tau, self.L)} # shadow manifold
#     for t in range((self.E-1) * self.tau, self.L):
#         x_lag = [] # lagged values
#         for t2 in range(0, self.E-1 + 1): # get lags, we add 1 to E-1 because we want to include E
#             x_lag.append(X[t-t2*self.tau])            
#         M[t] = x_lag
#     return M

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# 
# # get pairwise distances between vectors in X
# def get_distances(self, Mx):
#     """
#     Args
#         Mx: The shadow manifold from X
#     Returns
#         t_steps: timesteps
#         dists: n x n matrix showing distances of each vector at t_step (rows) from other vectors (columns)
#     """
# 
#     # we extract the time indices and vectors from the manifold Mx
#     # we just want to be safe and convert the dictionary to a tuple (time, vector)
#     # to preserve the time inds when we separate them
#     t_vec = [(k, v) for k,v in Mx.items()]
#     t_steps = np.array([i[0] for i in t_vec])
#     vecs = np.array([i[1] for i in t_vec])
#     dists = distance.cdist(vecs, vecs)    
#     return t_steps, dists

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# 
# def get_nearest_distances(self, t, t_steps, dists):
#     """
#     Args:
#         t: timestep of vector whose nearest neighbors we want to compute
#         t_teps: time steps of all vectors in Mx, output of get_distances()
#         dists: distance matrix showing distance of each vector (row) from other vectors (columns). output of get_distances()
#         E: embedding dimension of shadow manifold Mx 
#     Returns:
#         nearest_timesteps: array of timesteps of E+1 vectors that are nearest to vector at time t
#         nearest_distances: array of distances corresponding to vectors closest to vector at time t
#     """
#     t_ind = np.where(t_steps == t) # get the index of time t
#     dist_t = dists[t_ind].squeeze() # distances from vector at time t (this is one row)
#     
#     # get top closest vectors
#     nearest_inds = np.argsort(dist_t)[1:self.E+1 + 1] # get indices sorted, we exclude 0 which is distance from itself
#     nearest_timesteps = t_steps[nearest_inds] # index column-wise, t_steps are same column and row-wise 
#     nearest_distances = dist_t[nearest_inds]  
#     
#     return nearest_timesteps, nearest_distances
#

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# def predict(self, t):
#     """
#     Args
#         t: timestep at Mx to predict Y at same time step
#     Returns
#         Y_true: the true value of Y at time t
#         Y_hat: the predicted value of Y at time t using Mx
#     """
#     eps = 0.000001 # epsilon minimum distance possible
#     t_ind = np.where(self.t_steps == t) # get the index of time t
#     dist_t = self.dists[t_ind].squeeze() # distances from vector at time t (this is one row)    
#     nearest_timesteps, nearest_distances = self.get_nearest_distances(t, self.t_steps, self.dists)    
#     
#     # get weights
#     u = np.exp(-nearest_distances/np.max([eps, nearest_distances[0]])) # we divide by the closest distance to scale
#     w = u / np.sum(u)
#     
#     # get prediction of X
#     X_true = self.X[t] # get corresponding true X
#     X_cor = np.array(self.X)[nearest_timesteps] # get corresponding Y to cluster in Mx
#     X_hat = (w * X_cor).sum() # get X_hat
#     
#     return X_true, X_hat
#     
#

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# def causality(self):
#     '''
#     Args:
#         None
#     Returns:
#         correl: how much self.X causes self.Y. correlation between predicted Y and true Y
#     '''
# 
#     # run over all timesteps in M
#     # X causes Y, we can predict X using My
#     # X puts some info into Y that we can use to reverse engineer X from Y        
#     X_true_list = []
#     X_hat_list = []
# 
#     for t in list(self.My.keys()): # for each time step in My
#         X_true, X_hat = self.predict(t) # predict X from My
#         X_true_list.append(X_true)
#         X_hat_list.append(X_hat) 
# 
#     x, y = X_true_list, X_hat_list
#     r, p = pearsonr(x, y)        
# 
#     return r, p
#





# print_causal_directions(cdc, 100, labels=labels)

"""##snowfall and sea_ice_extent in the artic regions##	
We investigate the bi-directional effects of the quantity of snow fall on the content of sea ice for the period between 1979 and 2021. 

Do we expect these two to be causally linked?
"""

#df1_norm['date'] = df1_norm.index

#df1_norm.columns

#df3 = df1_norm.loc['2015-01-01':'2021-08-31']

# #df['date'] = range(1999, 2010)
# make_plots(df, 'date', 'sea_ice_extent', 'snowfall')
## Helper plotting function
def make_plots(df, dt_name, val1_name, val2_name):
    # drop nulls
    df = df[[dt_name, val1_name, val2_name]].dropna()
    
    # smoothen
    date_smooth = np.linspace(df[dt_name].min(), df[dt_name].max(), 100) 
    spl = make_interp_spline(df[dt_name], df[val1_name], k=2)
    val1 = spl(date_smooth)
    spl = make_interp_spline(df[dt_name], df[val2_name], k=2)
    val2 = spl(date_smooth)    
    r, p = np.round(pearsonr(df[val1_name], df[val2_name]), 4)
    
    # plot
    f, ax = plt.subplots(figsize=(12, 4))
    ax.plot(date_smooth, val1, )
    ax = df.plot(x=dt_name, y=val1_name, marker='', c='b', linestyle='', legend=False, ax=ax)
    ax.set_ylabel(val1_name)
    ax2 = ax.twinx()
    ax2.plot(date_smooth, val2, c='r')
    df.plot(x=dt_name, y=val2_name, marker='', c='r', linestyle='', legend=False, ax=ax2)
    ax2.set_ylabel(val2_name)    
    ax.figure.legend()        
    plt.title(f"{val1_name} and {val2_name}, correlation coefficient: {r}", size=16)
    plt.tight_layout()
    plt.show()

df = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv')



df1 = df[['S2', 'S5']]
a = df1[['S2']].values
b = df1[['S5']].values
#df['date'] = df.index
make_plots(df1, df1.index, a, b)

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df1.shape[0] # length of time period to consider

Y = df1['S2'].values
X = df1['S5'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='snowfall - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('snowfall -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> snowfall r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""##wind_10m vs sea_ice_extent##

###Here we investigate the effects of wind speed at 10m above sea level to the content of sea ice###
"""

df = df3[['wind_10m', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, df.index, 'wind_10m', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['wind_10m'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='wind_10m - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('wind_10m -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> wind_10m r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""#**sw_down vs sea_ice_extent**#"""

df = df3[['SW_down', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, 'date', 'SW_down', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['SW_down'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='SW_down - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('SW_down -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> SW_down r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""#### SST vs sea_ice_extent"""

df = df3[['sst', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, 'date', 'sst', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['sst'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='sst - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('sst -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> sst r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))



"""### Functions for Visualizing"""

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# 
# def visualize_cross_mapping(self):
#     """
#     Visualize the shadow manifolds and some cross mappings
#     """
#     # we want to check cross mapping from Mx to My and My to Mx
# 
#     f, axs = plt.subplots(1, 2, figsize=(12, 6))        
#     
#     for i, ax in zip((0, 1), axs): # i will be used in switching Mx and My in Cross Mapping visualization
#         #===============================================
#         # Shadow Manifolds Visualization
# 
#         X_lag, Y_lag = [], []
#         for t in range(1, len(self.X)):
#             X_lag.append(X[t-tau])
#             Y_lag.append(Y[t-tau])    
#         X_t, Y_t = self.X[1:], self.Y[1:] # remove first value
# 
#         ax.scatter(X_t, X_lag, s=5, label='$M_x$')
#         ax.scatter(Y_t, Y_lag, s=5, label='$M_y$', c='y')
# 
#         #===============================================
#         # Cross Mapping Visualization
# 
#         A, B = [(self.Y, self.X), (self.X, self.Y)][i]
#         cm_direction = ['Mx to My', 'My to Mx'][i]
# 
#         Ma = self.shadow_manifold(A)
#         Mb = self.shadow_manifold(B)
# 
#         t_steps_A, dists_A = self.get_distances(Ma) # for distances between points in manifold
#         t_steps_B, dists_B = self.get_distances(Mb) # for distances between points in manifold
# 
#         # Plot cross mapping for different time steps
#         timesteps = list(Ma.keys())
#         for t in np.random.choice(timesteps, size=3, replace=False):
#             Ma_t = Ma[t]
#             near_t_A, near_d_A = self.get_nearest_distances(t, t_steps_A, dists_A)
# 
#             for i in range(E+1):
#                 # points on Ma
#                 A_t = Ma[near_t_A[i]][0]
#                 A_lag = Ma[near_t_A[i]][1]
#                 ax.scatter(A_t, A_lag, c='b', marker='s')
# 
#                 # corresponding points on Mb
#                 B_t = Mb[near_t_A[i]][0]
#                 B_lag = Mb[near_t_A[i]][1]
#                 ax.scatter(B_t, B_lag, c='r', marker='*', s=50)  
# 
#                 # connections
#                 ax.plot([A_t, B_t], [A_lag, B_lag], c='r', linestyle=':') 
# 
#         ax.set_title(f'{cm_direction} cross mapping. time lag, tau = {tau}, E = 2')
#         ax.legend(prop={'size': 14})
# 
#         ax.set_xlabel('$X_t$, $Y_t$', size=15)
#         ax.set_ylabel('$X_{t-1}$, $Y_{t-1}$', size=15)               
#     plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%add_to ccm
# def plot_ccm_correls(self):
#     """
#     Args
#         X: X time series
#         Y: Y time series
#         tau: time lag
#         E: shadow manifold embedding dimension
#         L: time duration
#     Returns
#         None. Just correlation plots
#     """
#     M = self.shadow_manifold(self.Y) # shadow manifold
#     t_steps, dists = self.get_distances(M) # for distances
# 
#     ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
#     ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X
# 
#     X_My_true, X_My_pred = [], [] # note pred X | My is equivalent to figuring out if X -> Y
#     Y_Mx_true, Y_Mx_pred = [], [] # note pred Y | Mx is equivalent to figuring out if Y -> X
# 
#     for t in range(tau, L):
#         true, pred = ccm_XY.predict(t)
#         X_My_true.append(true)
#         X_My_pred.append(pred)    
# 
#         true, pred = ccm_YX.predict(t)
#         Y_Mx_true.append(true)
#         Y_Mx_pred.append(pred)        
# 
#     # # plot
#     figs, axs = plt.subplots(1, 2, figsize=(12, 5))
#     
#     # predicting X from My
#     r, p = np.round(pearsonr(X_My_true, X_My_pred), 4)
#     
#     axs[0].scatter(X_My_true, X_My_pred, s=10)
#     axs[0].set_xlabel('$X(t)$ (observed)', size=12)
#     axs[0].set_ylabel('$\hat{X}(t)|M_y$ (estimated)', size=12)
#     axs[0].set_title(f'tau={tau}, E={E}, L={L}, Correlation coeff = {r}')
# 
#     # predicting Y from Mx
#     r, p = np.round(pearsonr(Y_Mx_true, Y_Mx_pred), 4)
#     
#     axs[1].scatter(Y_Mx_true, Y_Mx_pred, s=10)
#     axs[1].set_xlabel('$Y(t)$ (observed)', size=12)
#     axs[1].set_ylabel('$\hat{Y}(t)|M_x$ (estimated)', size=12)
#     axs[1].set_title(f'tau={tau}, E={E}, L={L}, Correlation coeff = {r}')
#     plt.show()
#

"""#Discovery Evaluation Metrics

We evaluate all the algorithms implemented on our sythethic data

#Implementation on Real World Dataset
"""

df1 = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", index_col='Date', parse_dates=True)
df1

df1.describe()

df1.dtypes

df2 = df1.values

# def mean(x): # np.mean(X, axis = 0)  
#     return sum(x)/len(x)  

# def std(x): # np.std(X, axis = 0)
#     return (sum((i - mean(x))**2 for i in x)/len(x))**0.5

# def Standardize_data(X):
#     return (X - mean(X))/std(X)

"""##Data Normalization - min-max feature scaling##"""

# copy the data
df1_norm = df1.copy()
  
# apply normalization techniques
for column in df1_norm.columns:
    df1_norm[column] = (df1_norm[column] - df1_norm[column].min()) / (df1_norm[column].max() - df1_norm[column].min())    
  
# view normalized data
print(df1_norm)



"""##Let's compute correlations between variables in our time series##
The correlation coefficient can be used to determine how multiple variables are associated with one another. The result is a correlation matrix that describes the correlation between time series variables. Note that the diagonal values in a correlation matrix will always be 1, since a time series will always be perfectly correlated with itself.

Correlation coefficients can be computed with the pearson, kendall and spearman methods. We use the pearson method because we assume that relationships between our variables are thought to be linear.
"""

df1_norm[['sea_ice_extent', 'wind_10m', "specific_humidity",	"LW_down",	"SW_down",	"rainfall",	"snowfall",	"sst",	"t2m",	"surface_pressure"]].corr(method='pearson')

sns.pairplot(df1_norm)

corr_df1_norm = df1_norm.corr(method='pearson')

# Customize the heatmap of the corr_meat correlation matrix
sns.heatmap(corr_df1_norm,
           annot=True,
           linewidths=0.4,
           annot_kws={'size': 10});

plt.xticks(rotation=90);
plt.yticks(rotation=0);
plt.savefig('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/Images/heatmap_all.png')

"""##Clustered heatmaps##
Heatmaps are extremely useful to visualize a correlation matrix, but clustermaps are better. A Clustermap allows to uncover structure in a correlation matrix by producing a hierarchically-clustered heatmap:
"""

corr_df1 = df1_norm.corr(method='pearson')

# Customize the heatmap of the corr_meat correlation matrix
fig = sns.clustermap(corr_df1,
               row_cluster=True,
               col_cluster=True,
               figsize=(10, 10));

plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90);
plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0);
plt.savefig('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/Images/clustermap.png')

"""#Data Preprocessing
###Handling of missing values
"""

#https://towardsdatascience.com/transformer-unleashed-deep-forecasting-of-multivariate-time-series-in-python-9ca729dac019
def missing(X):
  '''
  Input: X : The dataframe
  Output : When no missing values, it prints: no missing values
          When missing values are present, it prints them, and replaces them with neighbors
  '''
  if X.isnull().values.any():
    print("MISSING values:\n")
    mno.matrix(X)
    X = X.interpolate(method ="bfill")
    print(X)
  else:
    print("no missing values\n")

missing(df1_norm)

# # drop the NaN and zero columns, and also the 'forecast' columns
# df1 = df1.drop(df1.filter(regex="forecast").columns, axis=1, errors="ignore")
# df1.dropna(axis=1, how="all", inplace=True)
# df1 = df1.loc[:, (df1!=0).any(axis=0)]


# #handle missing values in rows of remaining columns
# df1 = df1.interpolate(method ="bfill")
# #any missing values left?
# gaps(df1)

# df1 = df1.loc[:, (df1!=0).any(axis=0)]

"""Outliers Detection """

# #https://towardsdatascience.com/transformer-unleashed-deep-forecasting-of-multivariate-time-series-in-python-9ca729dac019
# #boxplots
# for i, c in enumerate(df1_norm.columns):
#     sns.boxplot(x=df1_norm[c], palette="coolwarm")
#     plt.show();

"""###Should we drop outliars or replace them with interpolated values???

Approach 1. Quantile based flooring and capping

In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.
"""

#https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/

# for i in df1_norm.columns:

#   # Computing 10th, 90th percentiles and replacing the outliers
#   tenth_percentile = np.percentile(i, 10)
#   ninetieth_percentile = np.percentile(i, 90)
#   # print(tenth_percentile, ninetieth_percentile)
#   b = np.where(i<tenth_percentile, tenth_percentile, i)
#   b_i = np.where(b>ninetieth_percentile, ninetieth_percentile, b)
#   # print("Sample:", sample)
#   print("New array:",b_i)

#plt.plot(df1_norm.index, df1_norm[['sea_ice_extent']])

"""Approach 2. Mean/Median imputation

As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.
"""

# median = np.median(sample)# Replace with median
# for i in sample_outliers:
#     c = np.where(sample==i, 14, sample)
# print("Sample: ", sample)
# print("New array: ",c)
# # print(x.dtype)

"""#Time series Pre-processing

Visualize individual variables
"""

color_list = ["blue", "green", "orange", "red", "purple", "brown", "pink", "grey", "olive", "cyan" ]



def visual(df):
  features = list(df.select_dtypes(include=[np.number]).columns.values)
  feature_size = len(features)
  fig, axes = plt.subplots(nrows = int(np.ceil(feature_size/2)), ncols = 2, figsize = (14, feature_size*2), dpi = 80, facecolor = "w", edgecolor = "k")

  for i in range(feature_size):
    key = features[i]
    c = color_list[i % (len(color_list))]
    t_data = df[key]
    t_data.head()

    ax = t_data.plot(ax = axes[i // 2, i % 2],
    color = c,
    title = "{}".format(key),
    rot = 25)
    ax.legend([key])
  plt.tight_layout()

visual(df1_norm)

"""###Stationarity Check

A stationarity time series is a series whose statistical properties, such as mean, variance, etc., remain constant over time. In other words, its statistical properties are independent of the point in time at which they are observed.
On the other hand, a time series whose statistical properties change over time is called a non-stationary time series. Thus a time series with a trend or seasonality is non-stationary in nature. This is because the presence of trend or seasonality will affect the mean, variance and other properties at any given point in time.

It is important to stationarize our data so that the inferences drawn will be reliable because of a constant mean and variance. Analysis analysis or forecasts made using a non-stationary data are often erroneous and misleading.


##We use Statistical Tests (ADF) and Summary Statistics approaches

###Augmented Dickey-Fuller Test - This test relies on the following:
 
*   Null Hypothesis, H0: The time series is not stationary.
*   Alternative Hypothesis, H1: The time series is stationary.


*   If the p-value is less than or equal to 0.05 or the absolute value of the test statistics is greater than the critical value, you reject H0 and conclude that the time series is stationary.

*   If the p-value is greater than 0.05 or the absolute value of the test statistics is less than the critical value, you fail to reject H0 and conclude that the time series is not stationary.



*   From the results, the p-value is less than 0.05. So we reject the null hypothesis and conclude that the time series is stationary.
"""

#https://otexts.com/fpp2/stationarity.html

#https://blog.quantinsti.com/stationarity/
# Import adfuller
from statsmodels.tsa.stattools import adfuller

for i in df1_norm.columns:
  result = adfuller(df1_norm[[i]])
  print('')
  print('For variable: ', i)
  print('ADF Test Statistic: %.2f' % result[0])
  print('5%% Critical Value: %.2f' % result[4]['5%'])
  print('p-value: %.2f'  % result[1])

"""##Summary Statistics



*   Stationary time series has a constant mean, variance over time. Summary statistics like mean and variance are helpful in estimating whether a time series is stationary or not.
*   We partition the data into random periods and analyse the summary statistics for different periods. If the mean and variance of different partitions are very close to each other, the series is stationary.

*   If there is a significant difference between the mean and variance of the different partitions, then the series is not stationary.








"""

for i in df1_norm.columns:
  X = df1_norm[[i]]
  split = round(len(X) / 2)
  X1, X2 = X[0:split], X[split:]
  mean1, mean2 = X1.mean(), X2.mean()
  var1, var2 = X1.var(), X2.var()
  print('')
  print('For variable: ', i)
  print('mean1=%f, mean2=%f' % (mean1, mean2))
  print('variance1=%f, variance2=%f' % (var1, var2))

"""##Here we visualize individual variables to observe stationarity


"""

# fig,ax = plt.subplots(10, figsize=(15,10), sharex=True)
# plot_cols = ['wind_10m',	'specific_humidity',	'LW_down',	'SW_down',	'rainfall',	'snowfall',	'sst',	't2m',	'surface_pressure',	'sea_ice_extent']
# #plot_cols = ['wind_10m',	'SW_down',	'rainfall',	'snowfall',	'sst',	'sea_ice_extent']
# df1_norm[plot_cols].plot(subplots=True, legend=False, ax=ax)
# for a in range(len(ax)): 
#     ax[a].set_ylabel(plot_cols[a])
# ax[-1].set_xlabel('')
# plt.tight_layout()
# plt.show()

"""##Here we detrend our data"""

import statsmodels.api as sm
res = sm.tsa.seasonal_decompose(df1_norm["sea_ice_extent"], model = "additive")

for i in df1_norm.columns:
  data = []
  res = sm.tsa.seasonal_decompose(df1_norm[i], model = "additive")
  resplot = res.plot()
  print(resplot)

import os
import pickle
import numpy as np
import pandas as pd
from tqdm import tqdm
from numpy.linalg import pinv
from matplotlib import pyplot as plt
from matplotlib import colors as mcolors
from datetime import datetime, timedelta

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

colors = mcolors.CSS4_COLORS
colors = [x for x in list(colors.keys()) if 'blue' in x or 'turquoise' in x or 'cyan' in x]
colors.sort()
colors = colors[::-1]
print(colors)

def get_test_train_split(df, split=0.2):
    split_nb = int(len(df)*split)
    train, test = df.iloc[:-split_nb+1], df.iloc[-split_nb:]
    train.index = pd.to_datetime(train.index)
    test.index = pd.to_datetime(test.index)
    return train, test

def eliminate_trends_rolling_mean(df):
    roll = df.rolling(4).mean()
    avg_diff = (df - roll)/roll
    avg_diff.dropna(inplace=True)
    return avg_diff

def single_order_differencing(df):
    diff = df - df.shift()
    diff.dropna(inplace=True)
    return diff

def test_dickey_fuller_stationarity(df):
    rolmean = df.rolling(4).mean()
    rolstd = df.rolling(4).std()
    orig = plt.plot(df, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    dftest = adfuller(df, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic',
                                             'p-value',
                                             '#Lags Used',
                                             'Number of Observations Used'])
    conf = 0.
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
        if dfoutput['Test Statistic'] < dfoutput['Critical Value (%s)'%key]:
            dummy = 100 - int(key.split('%')[0])
            if dummy > conf:
                conf = dummy
    if conf > 95:
        return dfoutput, 'YES'
    else:
        return dfoutput, 'NO'

# df = process_week(df)
# df = process_df_by_week_univariate(df)
rm_df = eliminate_trends_rolling_mean(df1_norm.sea_ice_extent)
diff_df = single_order_differencing(rm_df)
_, res = test_dickey_fuller_stationarity(df1_norm.sea_ice_extent)
print('IS THE TIME SERIES STATIONARY? - ', res)
_, res = test_dickey_fuller_stationarity(rm_df)
print('IS THE TREND ELIMINATED TIME SERIES STATIONARY? - ', res)
_, res = test_dickey_fuller_stationarity(diff_df)
print('IS THE TREND ELIMINATED AND DIFFERENCED TIME SERIES STATIONARY? - ', res)

"""##Lets apply Smoothing to take care of the residuals"""



def exponential_smoothing(df, smoothing_level=0.2):
    model = SimpleExpSmoothing(np.asarray(df))
    model._index = pd.to_datetime(df.index)
    fit = model.fit(smoothing_level=smoothing_level)   
    return fit

def holts_linear_smoothing(df, smoothing_level=0.3, smoothing_slope=0.05):
    model = Holt(np.asarray(df))
    model._index = pd.to_datetime(df.index)
    fit = model.fit(smoothing_level=smoothing_level, smoothing_slope=smoothing_slope)   
    return fit
    
def holts_exponential_smoothing(df, trend=None, damped=False, seasonal=None, seasonal_periods=None):
    model = ExponentialSmoothing(np.asarray(df), trend=trend, seasonal=seasonal, damped=damped, seasonal_periods=seasonal_periods)
    model._index = pd.to_datetime(df.index)
    fit = model.fit()
    return fit

def smoothing_experiments(train, test, params, method, plot_dir):
    methods = ['simpl_exp', 'holts_lin', 'holts_exp']
    models = {}
    preds = {}
    rmse = {}
    if method == methods[0]:
        for s in params['smoothing_level']:
            models[s] = exponential_smoothing(train, smoothing_level=s)
            preds[s] = models[s].predict(start=1,end=len(test))
            preds[s] -= preds[s][0]
            preds[s] += train.values[-1]
            rmse[s] = np.sqrt(mean_squared_error(test, preds[s]))
    elif method == methods[1]:
        for sl in params['smoothing_level']:
            for ss in params['smoothing_slope']:
                models[(sl,ss)] = holts_linear_smoothing(train, smoothing_level=sl, smoothing_slope=ss)
                preds[(sl,ss)] = models[(sl,ss)].predict(start=1,end=len(test))                
                preds[(sl,ss)] -= preds[(sl,ss)][0]
                preds[(sl,ss)] += train.values[-1]
                rmse[(sl,ss)] = np.sqrt(mean_squared_error(test, preds[(sl,ss)]))
    elif method == methods[2]:
        for t in params['trend']:
            for d in params['damped']:
                models[(t,d)] = holts_exponential_smoothing(train, trend=t, damped=d)
                preds[(t,d)] = models[(t,d)].predict(start=1,end=len(test))
                preds[(t,d)] -= preds[(t,d)][0]
                preds[(t,d)] += train.values[-1]
                rmse[(t,d)] = np.sqrt(mean_squared_error(test, preds[(t,d)]))
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(train.index, train.values, color="gray")
    ax.plot(test.index, test.values, color="gray")
    for p, f, r, c in zip(list(preds.values()),list(models.values()),list(rmse.values()),colors[:len(preds)]):
#         ax.plot(train.index, f.fittedvalues, color=c)
        if method == methods[0]:
            ax.plot(test.index, p, label="alpha="+str(f.params['smoothing_level'])[:3]+" RMSE: "+str(r), color=c)
            ax.set_title("Simple Exponential Smoothing")    
            ax.legend();
        elif method == methods[1]:
            ax.plot(test.index, p, label="alpha="+str(f.params['smoothing_level'])[:4]+", beta="+str(f.params['smoothing_slope'])[:4]+" RMSE: "+str(r), color=c)
            ax.set_title("Holts Linear Smoothing")    
            ax.legend();
        elif method == methods[2]:
            ax.plot(test.index, p, 
                    label="alpha="+str(f.params['smoothing_level'])[:4]+", beta="+str(f.params['smoothing_slope'])[:4]+ ", damping="+str(True if f.params['damping_slope']>0 else False)+" RMSE: "+str(r), 
                    color=c)
            ax.set_title("Holts Exponential Smoothing")    
            ax.legend();
    plot_name = method + '.png'
    fig.savefig(os.path.join(plot_dir, plot_name))
    return models, preds, rmse

simpl_exp_params = {
    'smoothing_level': [0.2, 0.3, 0.5],
}

holts_lin_params = {
    'smoothing_level': [0.2, 0.3, 0.5],
    'smoothing_slope': [0.05, 0.1, 0.2],
}

holts_exp_params = {
    'trend': ['add'],
    'damped': [False, True],
}



this_dir = os.getcwd()
parent_dir = os.path.dirname(os.path.normpath(this_dir))
plots_dir = os.path.join(parent_dir, 'plots', 'forecasting')
preproc_dir = os.path.join(parent_dir, 'preproc_files')
f_store_dfs = os.path.join(preproc_dir, 'store_dfs')
if not os.path.exists(plots_dir):
    os.makedirs(plots_dir)

train, test = get_test_train_split(df1_norm, split=0.2)
plots_dir = "/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/Images/"
smoothing_plots_dir = os.path.join(plots_dir, 'smoothing')
smoothing_exp_dir = os.path.join(smoothing_plots_dir, 'experiments')
if not os.path.exists(smoothing_exp_dir):
    os.makedirs(smoothing_exp_dir)

simpl_exp_models, simpl_exp_preds, simpl_exp_rmse = smoothing_experiments(train, test, simpl_exp_params, 'simpl_exp', smoothing_exp_dir)
holts_lin_models, holts_lin_preds, holts_lin_rmse = smoothing_experiments(train, test, holts_lin_params, 'holts_lin', smoothing_exp_dir)
holts_exp_models, holts_exp_preds, holts_exp_rmse = smoothing_experiments(train, test, holts_exp_params, 'holts_exp', smoothing_exp_dir)

split_date = '2014-01-01'
df_training = df1_norm.loc[df1_norm.index <= split_date]
df_test = df1_norm.loc[df1_norm.index > split_date]

# Walk throught the test data, training and predicting 1 day ahead for all the test data
index = len(df_training)
yhat = list()
for t in tqdm(range(len(df_test.pollution_today))):
    temp_train = air_pollution[:len(df_training)+t]
    model = SimpleExpSmoothing(temp_train.pollution_today)
    model_fit = model.fit()
    predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))
    yhat = yhat + [predictions]

yhat = pd.concat(yhat)
resultsDict['SES'] = evaluate(df_test.pollution_today, yhat.values)
predictionsDict['SES'] = yhat.values

#resplot = res.plot()

plt.figure(figsize=(22,7))
res.trend.plot()

res.observed #(trend + seasonal + residual)

print(res.trend)
#Here we see that the initial values are null because to find the trend, internally the algorithm does a backdated analysis of t-1 and t-2, so the first 2 and last 2 values are not found

print(res.seasonal)

print(res.resid)

#To retrieve our original additive time series we add all the components
res.observed[2]

pd.DataFrame(res.observed - res.trend).plot()

pd.DataFrame(res.observed - res.seasonal).plot()







"""Out dataset looks like its stationary since we can observe a constant variance. Also data points seem to always return towards the long-run mean. But just looking with the bear eye can't reveal some hidden aspects. Let's consider statistical testing with a parametric approach to detect stationarity:

Here we randomly pick a varible to examine all its time series components and observe the following:


*   It contains no seasonal or cyclical trend
*   It contains zero residuals
"""



plt.figure(figsize=(12,5))
#series = [i+randrange(10) for i in range(1,100)]
df2 = df1_norm[['sea_ice_extent']]
result = seasonal_decompose(df2, model='additive', period=1)

trend = result.trend
seasonal = result.seasonal
residual = result.resid

fig = result.plot()
fig.set_figwidth(8)
fig.set_figheight(5)
fig.suptitle('Decomposition of sea ice extent time series')
plt.show()

#pyplot.show()

# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler(feature_range=(0,1))
# array = scaler.fit_transform(np.array(df).reshape(-1,1))

"""###Important Links###


*   https://www.kaggle.com/code/abhishekmamidi/time-series-preprocessing-to-modelling/notebook

*   https://towardsdatascience.com/fun-with-arma-var-and-granger-causality-6fdd29d8391c



*   https://phdinds-aim.github.io/time_series_handbook/06_ConvergentCrossMappingandSugiharaCausality/ccm_sugihara.html#

Feature Engineering

So our algorithms can run smoothly, we select variables which are highly correlated with our response or outcome variable.
"""

df1_norm

# df1_norms = df1_norm.index.rename('date', inplace=True)
# df1_norms

#Here we consider only attributes with corr greater than 0.6
idxs = []

for i, corr in enumerate(corr_df1_norm['sea_ice_extent']):
    if corr > 0.1 and pd.notna(corr):
        idxs.append(i)
    else:
      corr < -0.71 and pd.notna(corr)
      idxs.append(i)


idxs = idxs[1:]
sel_feat = corr_df1_norm['sea_ice_extent'][idxs]
sel_feat

df3 = df1_norm.loc['2015-01-01':'2021-08-31']

# fig,ax = plt.subplots(6, figsize=(15,8), sharex=True)
# #plot_cols = ['wind_10m',	'specific_humidity',	'LW_down',	'SW_down',	'rainfall',	'snowfall',	'sst',	't2m',	'surface_pressure',	'sea_ice_extent']
# plot_cols = ['wind_10m',	'SW_down',	'rainfall',	'snowfall',	'sst',	'sea_ice_extent']
# df3[plot_cols].plot(subplots=True, legend=False, ax=ax)
# for a in range(len(ax)): 
#     ax[a].set_ylabel(plot_cols[a])
# ax[-1].set_xlabel('')
# plt.tight_layout()
# plt.show()

# df3 = df3[['wind_10m',	'SW_down',	'rainfall',	'snowfall',	'sst',	'sea_ice_extent']]
# corr_df1_norm = df3.corr(method='pearson')

# # Customize the heatmap of the corr_meat correlation matrix
# sns.heatmap(corr_df1_norm,
#            annot=True,
#            linewidths=0.4,
#            annot_kws={'size': 10});

# plt.xticks(rotation=90);
# plt.yticks(rotation=0);

# df = df3[['sea_ice_extent', 'snowfall']]
# df

"""#Model Application

*   Baseline Models: Granger Causality, Temporal Causal Discovery Framework(TCDF)
*   Our Model: Convergence Cross Mapping

##Generate a causal Graph from our data

###From CausalModel
"""

#!pip install dowhy

#https://towardsdatascience.com/4-python-packages-to-learn-causal-analysis-9a8eaab9fdab

# from dowhy import CausalModel

# #From causal Model
# model= CausalModel(
#         data = df1_norm,
#         graph=causal_graph.replace("\n", " "),
#         treatment='snowfall',
#         outcome='sea_ice_extent')
# model.view_model()
# from IPython.display import Image, display
# display(Image(filename="causal_model.png"))



"""###From PC Algorithm"""

data1 = df1_norm.values

cg = pc(data1, 0.05, fisherz, True, 0, -1)

# visualization using pydot
cols = df1_norm.columns
cg.draw_pydot_graph(labels=cols)

from causallearn.search.ConstraintBased.FCI import fci

G, edges = fci(data1, fisherz, 0.05, -1, -1, )

# visualization
from causallearn.utils.GraphUtils import GraphUtils
cols = df1_norm.columns
pdy = GraphUtils.to_pydot(G, labels=cols)
#pdy.write_png('simple_test.png')

plt.figure(figsize=(12,8))

# Visualization using pydot
#pyd = GraphUtils.to_pydot(Record['G'], labels=cols)
tmp_png = pdy.create_png(f="png")
fp = io.BytesIO(tmp_png)
img = mpimg.imread(fp, format='png')
plt.axis('off')
plt.imshow(img)
plt.show()

"""###From GES"""

Record = ges(data1, score_func = "local_score_BIC", maxP = None, parameters = None)

# Visualization using pydot
from causallearn.utils.GraphUtils import GraphUtils
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import io



pyd = GraphUtils.to_pydot(Record['G'], labels=cols)
tmp_png = pyd.create_png(f="png")
fp = io.BytesIO(tmp_png)
img = mpimg.imread(fp, format='png')
plt.axis('off')
plt.imshow(img)
plt.show()

"""#*Granger Causality Test*#

Linear regression might indicate a strong relationship between two or more variables, but these variables may be totally unrelated in reality. Predictions fail when it comes to domain knowledge, this scenario is known as spurious regression.

There is a strong relationship between chicken consumption and crude oil exports in the below graph even though they are unrelated.

A prerequisite for performing the Granger Causality test is that the data need to be stationary i.e it should have a constant mean, constant variance, and no seasonal component. Transform the non-stationary data to stationary data by differencing it, either first-order or second-order differencing. Do not proceed with the Granger causality test if the data is not stationary after second-order differencing.

Let us consider three variables Xt , Yt , and Wt preset in time series data

Forecast Xt+1 based on past values Xt and Yt

Let's assume from our causal graph that the past values of Y contain information for forecasting Xt+1. Yt is said to “Granger cause” Xt+1 provided Yt occurs before Xt+1 and it contains data for forecasting Xt+1

If Yt causes Xt, then Y must precede X which implies:
*   Lagged values of Y should be significantly related to X.
*   Lagged values of X should not be significantly related to Y.



"""

#Nice resource: https://causal-learn.readthedocs.io/en/latest/search_methods_index/Granger%20causality/LinearGranger.html
#https://www.analyticsvidhya.com/blog/2021/08/granger-causality-in-time-series-explained-using-chicken-and-egg-problem/
#https://www.youtube.com/watch?v=4TkNZviNJC0



data = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv')
df = data.drop('Date', axis=1)
#data = np.array(df)

n_obs=len(df) - 13000
X_train, X_test = df[0:-n_obs], df[-n_obs:]
print(X_train.shape, X_test.shape)

maxlag=12
test = 'ssr-chi2test'
def grangers_causality_matrix(X_train, variables, test = 'ssr_chi2test', verbose=False):
  dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
  for c in dataset.columns:
    for r in dataset.index:
      test_result = grangercausalitytests(X_train[[r,c]], maxlag=maxlag, verbose=False)
      p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
      if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
      
      min_p_value = np.min(p_values)
      dataset.loc[r,c] = min_p_value
  dataset.columns = [var + '_x' for var in variables]
  dataset.index = [var + '_y' for var in variables]
  return dataset
grangers_causality_matrix(df, variables = df.columns)

"""As evident from the results. The p-value of S1_x for S2_y, S3_y, S4_y, S5_y, S6_y, S7_y and S8_y is less than 0.05, which means the Granger test identifies S1 to be the cause of S2, S3 and S4. Similarly, S3 is identified to be the granger cause of S4.

#PCMCI
This methods combines the PC stable causal discovery algorithm with momentary conditional independence MCI to estimate causal networks from large-scale time series datasets in a two-step approach. The first step is to find parents of all individual nodes using the PC stable method. The second step is to test the momentary conditional independence. As a result of the second step, if two nodes are independent, then there exists no cause-effect relationship between them. The outcome of this two-step method is a causal adjacency matrix, the corresponding lag values and strength given by p-values.
Source: https://github.com/jakobrunge/tigramite
"""

# #Restart runtime after successfully running this cell

# !pip install dcor
# !pip install matplotlib --upgrade
# #matplotlib.__version__
# !pip install tigramite

# # Imports
# import numpy as np
# import matplotlib
# import pandas as pd
# from matplotlib import pyplot as plt
# %matplotlib inline     
# import sklearn
# import tigramite
# from tigramite import data_processing as pp
# from tigramite.toymodels import structural_causal_processes as toys
# from tigramite import plotting as tp
# from tigramite.pcmci import PCMCI
# from tigramite.independence_tests import ParCorr, GPDC, CMIknn, CMIsymb

df1 = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv')
df = df1.drop('Date', axis=1)
data = np.array(df)
var_names=['wind_10m', 'specific_humidity', 'LW_down', 'SW_down', 'rainfall', 'snowfall',	'sst', 't2m',	'surface_pressure'	, 'sea_ice_extent']

dataframe = pp.DataFrame(data, 
                         datatime = {0:np.arange(len(data))}, 
                         var_names=var_names)

#Define the maximum lag and conditional independence test. 
max_lag = 5
parcorr = ParCorr(significance='analytic')
#Create an object of PCMCI class
pcmci = PCMCI(
    dataframe=dataframe, 
    cond_ind_test=parcorr,
    verbosity=1)

pcmci.verbosity = 1
results = pcmci.run_pcmci(tau_max=max_lag, pc_alpha=None, alpha_level=0.01)

"""#### Plotting
The network is defined from links in graph. Nodes denote variables, straight links contemporaneous dependencies and curved arrows lagged dependencies. The node color denotes the maximal absolute auto-dependency and the link color the value at the lag with maximal absolute cross-dependency. The link label lists the lags with significant dependency in order of absolute magnitude.
"""

tp.plot_graph(
    val_matrix=results['val_matrix'],
    graph=results['graph'],
    var_names=var_names,
    link_colorbar_label='cross-MCI',
    node_colorbar_label='auto-MCI',
    ); plt.show()

"""From this graph, we see that PCMCI correctly identifies the time-lag that is '1' and also identifies the causal relations between
- S1 and S2, S3, S4, S5, S6, 
- S3 and S5, S6
- S4 and S7, S5

#VarLiNGAM

VarLiNGAM is a Structural Equation Models (SEM) method of Causal Discovery. It combines the basic LiNGAM model with the classic vector autoregressive models (VAR). It enables analyzing both lagged and contemporaneous (instantaneous) causal relations. Therefore it is allowed to discover linear instantaneous and lagged effects of time-series data.
Please found the documentation of VarLiNGAM: https://lingam.readthedocs.io/en/latest/tutorial/var.html
"""

# # Install libraries 
# !pip install lingam
# !pip install igraph
# !pip install factor_analyzer
# !pip install pygam

# # Import libraries
# import numpy as np
# import pandas as pd
# import graphviz
# import lingam
# from lingam.utils import make_dot, print_causal_directions, print_dagc

# Read Data and remove the first line of variable names
#X = pd.read_csv('/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/synthetic_data_nonlinear_ff.csv',skiprows = 1, header=None)

# Initialize the model and fit data into the model
# It takes about 4 minitues to run this model with this ysnthetic data
model = lingam.VARLiNGAM(lags=5, criterion=None, prune=True)
model.fit(df)

"""#### Result
VarLiNGAM reports causal relationships in adjacency matrices. The elements in the matrices $B$ are causal effects.

How to read adjacency matrices:



We have 4 input varaibles: {$S_1$, $S_2$, $S_3$, $S_4$}, and the maximum time delay is 5 (see the last cell). Therefore we have six (6=5+1) $4\times 4$ matrices: {$B_0$, $B_1$, $B_2$, $B_3$, $B_4$, $B_5$}. The $p$ of $B_p$ is time delay. Given a variable $x_t$, the column represents cause varaible at the time $t-p$, and the row represents the effect variable $x-t$. For example, if we have a matrix $B_0$ and $B_1$ as below:
![unnamed.png](data:image/png;base64,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)

Then it means that there is an instantaneous effect $x^2_t$ -> $x^1_t$, and no lagged effects $x^2_{t-1}$ -> $x^1_t$ (t)



"""

# Print adjacent matrices
print(model.adjacency_matrices_)

"""From the results we can see, VarLiNGAM believes there only causal relationships when time lag is up to 1. It is reasonable, according to our synthetic data.

Let's print a causal plot. The $lower limit = 0.05$ means we only look at causal edges with a causal effect larger than 0.05 and ignore other edges in the graph. 
"""

new_matrices = np.stack((model.adjacency_matrices_[0], model.adjacency_matrices_[1]))
labels = ['wind_10m(t)', 'specific_humidity(t)', 'LW_down(t)', 'SW_down(t)', 'rainfall(t)', 'snowfall(t)', 'sst(t)', 't2m(t)', 'surface_pressure(t)', 'sea_ice_extent(t)',
          'wind_10m(t-1)', 'specific_humidity(t-1)', 'LW_down(t-1)', 'SW_down(t-1)', 'rainfall(t-1)', 'snowfall(t-1)', 'sst(t-1)', 't2m(t-1)', 'surface_pressure(t-1)', 'sea_ice_extent(t-1)']
make_dot(np.hstack(new_matrices), ignore_shape=True, lower_limit=0.05, labels=labels)

# def func_1(A, B, r, beta):
#     return A * (r - r * A - beta * B)

# !pip install jdc



# for i in df1_norm.columns:
#   print(i)

# for i in df1_norm.columns:
#   for j in df1_norm.columns:
#     if i == j:
#       res = grangercausalitytests(df1_norm[[i, j]], maxlag=4)
#     print("The Granger Causality results for", i, "and ", j)
#   print(res)

"""#*PCMCI Algorithm*#"""

df = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", index_col='Date', parse_dates=True)

data = np.array(df)
var_names=['wind_10m', 'specific_humidity', 'LW_down', 'SW_down', 'rainfall',
       'snowfall', 'sst', 't2m', 'surface_pressure', 'sea_ice_extent']

dataframe = pp.DataFrame(data, 
                         datatime = {0:np.arange(len(data))}, 
                         var_names=var_names)

#Define the maximum lag and conditional independence test. 
max_lag = 5
parcorr = ParCorr(significance='analytic')
#Create an object of PCMCI class
pcmci = PCMCI(
    dataframe=dataframe, 
    cond_ind_test=parcorr,
    verbosity=1)

pcmci.verbosity = 1
results = pcmci.run_pcmci(tau_max=max_lag, pc_alpha=None, alpha_level=0.01)

"""#### Plotting
The network is defined from links in graph. Nodes denote variables, straight links contemporaneous dependencies and curved arrows lagged dependencies. The node color denotes the maximal absolute auto-dependency and the link color the value at the lag with maximal absolute cross-dependency. The link label lists the lags with significant dependency in order of absolute magnitude.
"""

tp.plot_graph(
    val_matrix=results['val_matrix'],
    graph=results['graph'],
    var_names=var_names,
    link_colorbar_label='cross-MCI',
    node_colorbar_label='auto-MCI', 
    );
plt.show()



"""VarLiNGAM is a Structural Equation Models (SEM) method of Causal Discovery. It combines the basic LiNGAM model with the classic vector autoregressive models (VAR). It enables analyzing both lagged and contemporaneous (instantaneous) causal relations. Therefore it is allowed to discover linear instantaneous and lagged effects of time-series data.
Please found the documentation of VarLiNGAM: https://lingam.readthedocs.io/en/latest/tutorial/var.html 
"""

# Read Data and remove the first line of variable names
#df = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", skiprows = 1, header=None)
df1 = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", index_col='Date', parse_dates=True)

df= df1.drop(df1.columns[0], axis=1)

# copy the data
df1_norm = df.copy()
  
# apply normalization techniques
for column in df.columns:
    df1_norm[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())    
  
# view normalized data
print(df1_norm)

#X= df1_norm.drop(df1_norm.columns[0], axis=1)

# Initialize the model and fit data into the model
# It takes about 4 minitues to run this model with this ysnthetic data
model = lingam.VARLiNGAM(lags=5, criterion=None, prune=True)
model.fit(df1)

"""#### Result
VarLiNGAM reports causal relationships in adjacency matrices. 


"""

# Print adjacent matrices
print(model.adjacency_matrices_)

"""From the results we can see, VarLiNGAM believes there only causal relationships when time lag is up to 1. It is reasonable, according to our synthetic data.

Let's print a causal plot. The $lower limit = 0.05$ means we only look at causal edges with a causal effect larger than 0.05 and ignore other edges in the graph. 
"""

new_matrices = np.stack((model.adjacency_matrices_[0], model.adjacency_matrices_[1]))
labels = ['wind_10m(t)', 'specific_humidity(t)', 'LW_down(t)', 'SW_down(t)', 'rainfall(t)', 'snowfall(t)', 'sst(t)', 't2m(t)', 'surface_pressure(t)', 'sea_ice_extent(t)',
          'wind_10m(t-1)', 'specific_humidity(t-1)', 'LW_down(t-1)', 'SW_down(t-1)', 'rainfall(t-1)', 'snowfall(t-1)', 'sst(t-1)', 't2m(t-1)', 'surface_pressure(t-1)', 'sea_ice_extent(t-1)']
make_dot(np.hstack(new_matrices), ignore_shape=True, lower_limit=0.05, labels=labels)

df3 = df1_norm.loc['2017-01-01':'2021-08-31']
model = lingam.VARLiNGAM(lags=5, criterion=None, prune=True)
model.fit(df3)

p_values = model.get_error_independence_p_values()
print(p_values)

# model = lingam.VARLiNGAM()
result = model.bootstrap(df3, n_sampling=100)

cdc = result.get_causal_direction_counts(n_directions=8, min_causal_effect=0.3, split_by_causal_effect_sign=True)

print_causal_directions(cdc, 100, labels=labels)

"""If we compare with the ground truth.
The ground truth: 


VarLiNGAM discovery:

#*CCM*#

##snowfall and sea_ice_extent in the artic regions##	
We investigate the bi-directional effects of the quantity of snow fall on the content of sea ice for the period between 1979 and 2021. 

Do we expect these two to be causally linked?
"""



# Read Data and remove the first line of variable names
#df = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", skiprows = 1, header=None)
df1 = pd.read_csv("/content/drive/MyDrive/Fall_2022/IS_800_Causality_New/Team_Project/sea_ice1.csv", index_col='Date', parse_dates=True)

df= df1.drop(df1.columns[0], axis=1)

# copy the data
df1_norm = df.copy()
  
# apply normalization techniques
for column in df.columns:
    df1_norm[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())    
  
# view normalized data
print(df1_norm)

df1_norm['date'] = df1_norm.index

df1_norm.columns

df3 = df1_norm.loc['2015-01-01':'2021-08-31']

# #df['date'] = range(1999, 2010)
# make_plots(df, 'date', 'sea_ice_extent', 'snowfall')
## Helper plotting function
def make_plots(df, dt_name, val1_name, val2_name):
    # drop nulls
    df = df[[dt_name, val1_name, val2_name]].dropna()
    
    # smoothen
    date_smooth = np.linspace(df[dt_name].min(), df[dt_name].max(), 100) 
    spl = make_interp_spline(df[dt_name], df[val1_name], k=2)
    val1 = spl(date_smooth)
    spl = make_interp_spline(df[dt_name], df[val2_name], k=2)
    val2 = spl(date_smooth)    
    r, p = np.round(pearsonr(df[val1_name], df[val2_name]), 4)
    
    # plot
    f, ax = plt.subplots(figsize=(12, 4))
    ax.plot(date_smooth, val1, )
    ax = df.plot(x=dt_name, y=val1_name, marker='', c='b', linestyle='', legend=False, ax=ax)
    ax.set_ylabel(val1_name)
    ax2 = ax.twinx()
    ax2.plot(date_smooth, val2, c='r')
    df.plot(x=dt_name, y=val2_name, marker='', c='r', linestyle='', legend=False, ax=ax2)
    ax2.set_ylabel(val2_name)    
    ax.figure.legend()        
    plt.title(f"{val1_name} and {val2_name}, correlation coefficient: {r}", size=16)
    plt.tight_layout()
    plt.show()

df = df3[['snowfall', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, df.date, df.snowfall, df.sea_ice_extent)

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['snowfall'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='snowfall - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('snowfall -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> snowfall r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""##wind_10m vs sea_ice_extent##

###Here we investigate the effects of wind speed at 10m above sea level to the content of sea ice###
"""

df = df3[['wind_10m', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, df.index, 'wind_10m', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['wind_10m'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='wind_10m - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('wind_10m -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> wind_10m r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""#**sw_down vs sea_ice_extent**#"""

df = df3[['SW_down', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, 'date', 'SW_down', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['SW_down'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='SW_down - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('SW_down -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> SW_down r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))

"""#### SST vs sea_ice_extent"""

df = df3[['sst', 'sea_ice_extent']]
df['date'] = df.index
#make_plots(df, 'date', 'sst', 'sea_ice_extent')

# Visualize simple shadow manifolds Mx and My for different tau
# We visualize Cross-Mapping

np.random.seed(1) # we fix the seed when randomly choosing cross mapping points
tau = 1 # time lag
E = 2 # shadow manifold embedding dimensions
L = df.shape[0] # length of time period to consider

Y = df['sea_ice_extent'].values
X = df['sst'].values


ccm1 = ccm(X, Y, tau, E, L)

# causality X -> Y
# returns: (correlation ("strength" of causality), p-value(significance))
ccm1.causality()


# visualize sample cross mapping
ccm1.visualize_cross_mapping()

# Looking at "convergence"
L_range = range(3, L+1, 1) # L values to test
Xhat_My, Yhat_Mx = [], [] # correlation list
for L in L_range: 
    ccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y
    ccm_YX = ccm(Y, X, tau, E, L) # define new ccm object # Testing for Y -> X    
    Xhat_My.append(ccm_XY.causality()[0]) 
    Yhat_Mx.append(ccm_YX.causality()[0])    
    
# Check correlation plot
ccm1.plot_ccm_correls() 

# plot convergence as L->inf. Convergence is necessary to conclude causality
plt.figure(figsize=(16,9))
plt.plot(L_range, Xhat_My, label='sst - $\hat{X}(t)|M_y$')
plt.plot(L_range, Yhat_Mx, label='sea_ice_extent - $\hat{Y}(t)|M_x$')
plt.xlabel('L', size=12)
plt.ylabel('correl', size=12)
plt.legend(prop={'size': 16})  

print('sst -> sea_ice_extent r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 2))
print('sea_ice_extent -> sst r', np.round(Yhat_Mx[-1], 2), 'p value', np.round(ccm_YX.causality()[1], 2))